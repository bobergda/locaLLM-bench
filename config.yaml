# Configuration file for locaLLM-bench.
# See README.md for details.
ollama_host: "http://localhost:11434"

# List of models to benchmark.
models:
#  - "qwen3:0.6b"
#  - "qwen3:1.7b"
#  - "qwen3:8b"
  - "qwen3:14b"

# Directory containing test definitions (JSON files).
tests_dir: "tests"

# Runner selection and logging.
log_level: "DEBUG" # DEBUG|INFO|WARNING|ERROR
include_test_sets: [ "bobers_last_exam" ] # empty list = all
task_start_id: 0 # 0-based index of the task to start from within each test set
task_limit: 2 # how many tasks to run from task_start_id; null/0 = run to the end

# SECURITY: executing model-produced code is dangerous. Keep this false unless you trust `results.json`.
unsafe_code_exec: true

# Code test runner mode:
# - "safe": run in a separate process with timeout (slower, safer against hangs)
# - "fast": run inline without timeout (fast, can hang on `while True`)
code_test_mode: "safe"

# Hard timeout for `code_tests` execution (seconds). Timeouts fail the test.
code_test_timeout_s: 2.0

# Optional Ollama generation options passed as `options` to `client.generate(...)`.
# Common keys: temperature, seed, num_ctx, top_p, top_k, repeat_penalty.
generate_options: {}
